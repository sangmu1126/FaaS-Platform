import os
import shutil
import time
import zipfile
import structlog
import boto3
import docker
from pathlib import Path
from collections import deque
from dataclasses import dataclass, field
from typing import List, Optional, Dict
from datetime import datetime

logger = structlog.get_logger()

# --- Data Models ---
@dataclass
class TaskMessage:
    request_id: str
    function_id: str
    runtime: str
    s3_key: str
    memory_mb: int = 128
    timeout_ms: int = 10000

@dataclass
class ExecutionResult:
    request_id: str
    success: bool
    exit_code: int
    stdout: str
    stderr: str
    duration_ms: int
    peak_memory_bytes: Optional[int] = None
    optimization_tip: Optional[str] = None
    output_files: List[str] = field(default_factory=list)

    def to_dict(self):
        return {
            "requestId": self.request_id,
            "status": "SUCCESS" if self.success else "FAILED",
            "exitCode": self.exit_code,
            "stdout": self.stdout,
            "stderr": self.stderr,
            "durationMs": self.duration_ms,
            "peakMemoryBytes": self.peak_memory_bytes,
            "optimizationTip": self.optimization_tip,
            "outputFiles": self.output_files
        }

# --- Service Logic ---

class AutoTuner:
    """ë©”ëª¨ë¦¬ ìµœì í™” íŒ ìƒì„± (ë¹„ìš© ì ˆê° í•µì‹¬)"""
    @staticmethod
    def analyze(peak_bytes: int, allocated_mb: int) -> Optional[str]:
        if not peak_bytes: return None
        if allocated_mb <= 0: allocated_mb = 128  # 0 ë‚˜ëˆ„ê¸° ë°©ì–´

        peak_mb = peak_bytes // (1024 * 1024)
        ratio = peak_mb / allocated_mb
        
        if ratio < 0.3:
            rec = max(int(peak_mb * 1.5), 10) # ìµœì†Œ 10MB ê¶Œì¥
            saved_percent = int((1 - (rec / allocated_mb)) * 100)
            if saved_percent <= 0: return None
            return f"ğŸ’¡ Tip: ì‹¤ì œ ì‚¬ìš©ëŸ‰({peak_mb}MB)ì´ í• ë‹¹ëŸ‰({allocated_mb}MB)ë³´ë‹¤ í›¨ì”¬ ì ìŠµë‹ˆë‹¤. {rec}MBë¡œ ì¤„ì—¬ ë¹„ìš©ì„ ì•½ {saved_percent}% ì ˆê°í•˜ì„¸ìš”."
        elif ratio > 0.9:
            rec = int(peak_mb * 1.2)
            return f"âš ï¸ Warning: ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({peak_mb}MB). {rec}MB ì´ìƒìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        return None

class CloudWatchPublisher:
    """ASG ì—°ë™ì„ ìœ„í•œ CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡"""
    def __init__(self, region):
        self.client = boto3.client("cloudwatch", region_name=region)
        
    def publish_peak_memory(self, func_id, runtime, bytes_used):
        try:
            if bytes_used is None: return
            # ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ë©´ ë” ì¢‹ìŒ (ì—¬ê¸°ì„  ë¡œê¹…ë§Œ)
            # logger.debug("Publishing CloudWatch Metric", value=bytes_used)
            self.client.put_metric_data(
                Namespace="NanoGrid/FunctionRunner",
                MetricData=[{
                    "MetricName": "PeakMemoryBytes",
                    "Dimensions": [{"Name": "FunctionId", "Value": func_id}, {"Name": "Runtime", "Value": runtime}],
                    "Value": float(bytes_used),
                    "Unit": "Bytes",
                    "Timestamp": datetime.utcnow()
                }]
            )
        except Exception as e:
            logger.warning("CloudWatch publish failed", error=str(e))

class TaskExecutor:
    """í†µí•© ì‹¤í–‰ ì—”ì§„: S3 ë‹¤ìš´ë¡œë“œ -> Docker ì‹¤í–‰ -> ê²°ê³¼ ì²˜ë¦¬"""
    
    def __init__(self, config: Dict):
        self.cfg = config
        self.docker = docker.from_env()
        self.s3 = boto3.client("s3", region_name=config.get("AWS_REGION", "ap-northeast-2"))
        self.cw = CloudWatchPublisher(config.get("AWS_REGION", "ap-northeast-2"))
        
        # Warm Pool ì €ì¥ì†Œ
        self.pools = {
            "python": deque(), "cpp": deque(), "nodejs": deque()
        }
        self.images = {
            "python": config.get("DOCKER_PYTHON_IMAGE", "python:3.9-slim"),
            "cpp": config.get("DOCKER_CPP_IMAGE", "gcc:latest"),
            "nodejs": config.get("DOCKER_NODEJS_IMAGE", "node:18-alpine")
        }
        
        self._initialize_warm_pool()

    def _initialize_warm_pool(self):
        """Warm Pool ì´ˆê¸°í™” (Cold Start ì œê±°)"""
        counts = {
            "python": int(self.cfg.get("WARM_POOL_PYTHON_SIZE", 1)),
            "cpp": int(self.cfg.get("WARM_POOL_CPP_SIZE", 1)),
            "nodejs": int(self.cfg.get("WARM_POOL_NODEJS_SIZE", 1))
        }
        logger.info("ğŸ”¥ Initializing Warm Pools", counts=counts)
        
        for runtime, count in counts.items():
            for _ in range(count):
                self._create_warm_container(runtime)

    def _create_warm_container(self, runtime: str) -> str:
        try:
            img = self.images.get(runtime)
            # ë¬´í•œ ëŒ€ê¸° ì»¨í…Œì´ë„ˆ ì‹¤í–‰
            c = self.docker.containers.run(
                img, command="tail -f /dev/null", detach=True,
                # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ë§ˆìš´íŠ¸ (ì½”ë“œ ì‹¤í–‰ìš©) - ì½ê¸° ì „ìš©ìœ¼ë¡œ ë§ˆìš´íŠ¸í•˜ê±°ë‚˜ í•„ìš”í•œ ê²½ë¡œë§Œ ë§ˆìš´íŠ¸ ê¶Œì¥
                # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì „ì²´ ì‘ì—… ë£¨íŠ¸ë¥¼ ë§ˆìš´íŠ¸
                volumes={self.cfg["DOCKER_WORK_DIR_ROOT"]: {"bind": "/workspace", "mode": "rw"}},
                network_mode="none", # ì¸í„°ë„· ì°¨ë‹¨ (ë³´ì•ˆ)
                mem_limit="512m",    # ì»¨í…Œì´ë„ˆ í•˜ë“œ ë¦¬ë°‹
                cpu_quota=50000      # 0.5 CPU
            )
            c.pause()
            self.pools[runtime].append(c.id)
            return c.id
        except Exception as e:
            logger.error("Failed to create warm container", runtime=runtime, error=str(e))
            return None

    def _acquire_container(self, runtime: str):
        """Warm Poolì—ì„œ ì»¨í…Œì´ë„ˆ íšë“ (Unpause)"""
        target_runtime = runtime if runtime in self.pools else "python"
        
        # Poolì´ ë¹„ì–´ìˆìœ¼ë©´ ì¦‰ì‹œ ìƒì„± (ë™ê¸°)
        if not self.pools[target_runtime]:
            logger.warning("Pool empty, creating new container synchronously", runtime=target_runtime)
            cid = self._create_warm_container(target_runtime)
            # ë°©ê¸ˆ ë§Œë“ ê±´ append ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ pop í•´ì•¼ í•¨
            if not cid: raise RuntimeError("Failed to create container")
            
        try:
            cid = self.pools[target_runtime].popleft()
            c = self.docker.containers.get(cid)
            if c.status == 'paused':
                c.unpause()
            return c
        except Exception:
            # ì‹¤íŒ¨ ì‹œ(ì´ë¯¸ ì£½ì€ ì»¨í…Œì´ë„ˆ ë“±) ì¬ê·€ì ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
            return self._acquire_container(target_runtime)

    def _prepare_workspace(self, task: TaskMessage) -> Path:
        """S3 ë‹¤ìš´ë¡œë“œ ë° [ë³´ì•ˆ] Zip Slip ë°©ì§€ ì••ì¶• í•´ì œ"""
        local_dir = Path(self.cfg["DOCKER_WORK_DIR_ROOT"]) / task.request_id
        if local_dir.exists(): shutil.rmtree(local_dir)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        zip_path = local_dir / "code.zip"
        self.s3.download_file(self.cfg["S3_CODE_BUCKET"], task.s3_key, str(zip_path))
        
        # âœ… [FIX] Zip Slip ë°©ì§€ ì½”ë“œ ì ìš©
        with zipfile.ZipFile(zip_path, "r") as zf:
            for member in zf.namelist():
                # ìƒìœ„ ë””ë ‰í„°ë¦¬(../) ì ‘ê·¼ ì‹œë„ ì°¨ë‹¨
                target_path = (local_dir / member).resolve()
                if not str(target_path).startswith(str(local_dir.resolve())):
                    logger.warning("Zip Slip attempt detected", file=member)
                    continue
                
                # íŒŒì¼ ì¶”ì¶œ
                if member.endswith('/'):
                    target_path.mkdir(parents=True, exist_ok=True)
                else:
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    with zf.open(member) as source, open(target_path, "wb") as dest:
                        shutil.copyfileobj(source, dest)
        
        zip_path.unlink()
        return local_dir

    def run(self, task: TaskMessage) -> ExecutionResult:
        container = None
        host_work_dir = None
        start_time = time.time()
        
        try:
            # 1. ì‘ì—… ê³µê°„ ì¤€ë¹„
            host_work_dir = self._prepare_workspace(task)
            # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ (/workspace ê°€ ë°”ì¸ë“œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ ê·¸ í•˜ìœ„ ê²½ë¡œ ì‚¬ìš©)
            container_work_dir = f"/workspace/{task.request_id}"

            # 2. ì»¨í…Œì´ë„ˆ íšë“ (Warm Start)
            container = self._acquire_container(task.runtime)
            
            # 3. ì‹¤í–‰ ì»¤ë§¨ë“œ êµ¬ì„±
            cmd = []
            if task.runtime == "python": 
                cmd = ["python", f"{container_work_dir}/main.py"]
            elif task.runtime == "cpp": 
                # C++ì€ ì»´íŒŒì¼ í›„ ì‹¤í–‰
                cmd = ["sh", "-c", f"g++ {container_work_dir}/main.cpp -o {container_work_dir}/out && {container_work_dir}/out"]
            elif task.runtime == "nodejs": 
                cmd = ["node", f"{container_work_dir}/index.js"]

            # 4. ì‹¤í–‰ (Exec)
            logger.info("Exec command", cmd=cmd, container=container.id[:12])
            exit_code, output = container.exec_run(
                cmd, workdir="/workspace", demux=False
            )
            
            # âœ… [FIX] í•˜ë“œì½”ë”© ì œê±° & ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •
            try:
                stats = container.stats(stream=False)
                # cgroup v1: usage, v2: may need adaptation. Docker API usually handles conversion.
                usage = stats['memory_stats'].get('usage', 0)
                # ìºì‹œ ë©”ëª¨ë¦¬ ì œì™¸í•œ ì‹¤ì œ ì‚¬ìš©ëŸ‰ (RSS)ì´ ë” ì •í™•í•  ìˆ˜ ìˆìŒ.
                # stats_mem = stats['memory_stats'].get('stats', {})
                # usage = usage - stats_mem.get('cache', 0)
            except Exception as e:
                logger.warning("Failed to get metrics", error=str(e))
                usage = 0
            
            # 6. Auto-Tuning & CloudWatch
            tip = AutoTuner.analyze(usage, task.memory_mb)
            self.cw.publish_peak_memory(task.function_id, task.runtime, usage)
            
            output_str = output.decode('utf-8', errors='replace')

            return ExecutionResult(
                request_id=task.request_id,
                success=(exit_code == 0),
                exit_code=exit_code,
                stdout=output_str,
                stderr="",
                duration_ms=int((time.time() - start_time) * 1000),
                peak_memory_bytes=usage,
                optimization_tip=tip
            )

        except Exception as e:
            logger.error("Execution failed", error=str(e))
            return ExecutionResult(
                request_id=task.request_id, success=False, exit_code=-1,
                stdout="", stderr=str(e), duration_ms=int((time.time() - start_time) * 1000)
            )
            
        finally:
            # âœ… [FIX] ì˜¤ì—¼ëœ ì»¨í…Œì´ë„ˆëŠ” ì¬ì‚¬ìš©í•˜ì§€ ì•Šê³  íê¸°
            if container:
                try:
                    # Dirty Container Removal
                    container.remove(force=True)
                except: pass
            
            # íŒŒì¼ ì‚­ì œ
            if host_work_dir and host_work_dir.exists():
                try: shutil.rmtree(host_work_dir)
                except: pass