import os
import shutil
import time
import zipfile
import structlog
import boto3
import docker
from pathlib import Path
from collections import deque
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
from datetime import datetime

logger = structlog.get_logger()

# --- Data Models (ê¸°ì¡´ models.py í†µí•©) ---
@dataclass
class TaskMessage:
    request_id: str
    function_id: str
    runtime: str
    s3_key: str
    memory_mb: int = 128
    timeout_ms: int = 10000

@dataclass
class ExecutionResult:
    request_id: str
    success: bool
    exit_code: int
    stdout: str
    stderr: str
    duration_ms: int
    peak_memory_bytes: Optional[int] = None
    optimization_tip: Optional[str] = None
    output_files: List[str] = field(default_factory=list)

    def to_dict(self):
        return {
            "requestId": self.request_id,
            "status": "SUCCESS" if self.success else "FAILED",
            "exitCode": self.exit_code,
            "stdout": self.stdout,
            "stderr": self.stderr,
            "durationMs": self.duration_ms,
            "peakMemoryBytes": self.peak_memory_bytes,
            "optimizationTip": self.optimization_tip,
            "outputFiles": self.output_files
        }

# --- Service Logic ---

class AutoTuner:
    """ë©”ëª¨ë¦¬ ìµœì í™” íŒ ìƒì„± (ê¸°ì¡´ docker_service ë‚´ë¶€ ë¡œì§)"""
    @staticmethod
    def analyze(peak_bytes: int, allocated_mb: int) -> Optional[str]:
        if not peak_bytes: return None
        peak_mb = peak_bytes // (1024 * 1024)
        ratio = peak_mb / allocated_mb
        
        if ratio < 0.3:
            rec = max(int(peak_mb * 1.5), 10)
            return f"ğŸ’¡ Tip: ì‹¤ì œ ì‚¬ìš©ëŸ‰({peak_mb}MB)ì´ í• ë‹¹ëŸ‰({allocated_mb}MB)ë³´ë‹¤ í›¨ì”¬ ì ìŠµë‹ˆë‹¤. {rec}MBë¡œ ì¤„ì—¬ ë¹„ìš©ì„ ì ˆê°í•˜ì„¸ìš”."
        elif ratio > 0.9:
            rec = int(peak_mb * 1.2)
            return f"âš ï¸ Warning: ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({peak_mb}MB). {rec}MB ì´ìƒìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        return None

class CloudWatchPublisher:
    """CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡ (ê¸°ì¡´ cloudwatch_publisher.py)"""
    def __init__(self, region):
        self.client = boto3.client("cloudwatch", region_name=region)
        
    def publish_peak_memory(self, func_id, runtime, bytes_used):
        try:
            self.client.put_metric_data(
                Namespace="NanoGrid/FunctionRunner",
                MetricData=[{
                    "MetricName": "PeakMemoryBytes",
                    "Dimensions": [{"Name": "FunctionId", "Value": func_id}, {"Name": "Runtime", "Value": runtime}],
                    "Value": float(bytes_used),
                    "Unit": "Bytes",
                    "Timestamp": datetime.utcnow()
                }]
            )
        except Exception as e:
            logger.warning("CloudWatch publish failed", error=str(e))

class TaskExecutor:
    """í†µí•© ì‹¤í–‰ ì—”ì§„: S3 ë‹¤ìš´ë¡œë“œ -> Docker ì‹¤í–‰ -> ê²°ê³¼ ì²˜ë¦¬"""
    
    def __init__(self, config: Dict):
        self.cfg = config
        self.docker = docker.from_env()
        self.s3 = boto3.client("s3", region_name=config["AWS_REGION"])
        self.cw = CloudWatchPublisher(config["AWS_REGION"])
        
        # Warm Pool ì €ì¥ì†Œ
        self.pools = {
            "python": deque(), "cpp": deque(), "nodejs": deque()
        }
        self.images = {
            "python": config["DOCKER_PYTHON_IMAGE"],
            "cpp": config["DOCKER_CPP_IMAGE"],
            "nodejs": config["DOCKER_NODEJS_IMAGE"]
        }
        
        # ì´ˆê¸°í™” ì‹œ Warm Pool ìƒì„±
        self._initialize_warm_pool()

    def _initialize_warm_pool(self):
        """Warm Pool ì´ˆê¸°í™” (ê¸°ì¡´ warm_pool ë¡œì§)"""
        counts = {
            "python": int(self.cfg.get("WARM_POOL_PYTHON_SIZE", 0)),
            "cpp": int(self.cfg.get("WARM_POOL_CPP_SIZE", 0)),
            "nodejs": int(self.cfg.get("WARM_POOL_NODEJS_SIZE", 0))
        }
        logger.info("ğŸ”¥ Initializing Warm Pools", counts=counts)
        
        for runtime, count in counts.items():
            for _ in range(count):
                self._create_warm_container(runtime)

    def _create_warm_container(self, runtime: str) -> str:
        """ì»¨í…Œì´ë„ˆ ìƒì„± ë° Pause"""
        try:
            img = self.images.get(runtime)
            c = self.docker.containers.run(
                img, command="tail -f /dev/null", detach=True,
                volumes={self.cfg["TASK_BASE_DIR"]: {"bind": self.cfg["DOCKER_WORK_DIR_ROOT"], "mode": "rw"}},
                network_mode="none" # ë³´ì•ˆ ê²©ë¦¬
            )
            c.pause()
            self.pools[runtime].append(c.id)
            return c.id
        except Exception as e:
            logger.error("Failed to create warm container", runtime=runtime, error=str(e))
            return None

    def _acquire_container(self, runtime: str):
        """Warm Poolì—ì„œ ì»¨í…Œì´ë„ˆ íšë“ (Unpause)"""
        if runtime not in self.pools:
            raise ValueError(f"Unsupported runtime: {runtime}")
            
        if not self.pools[runtime]:
            logger.warning("Pool empty, creating new one", runtime=runtime)
            self._create_warm_container(runtime)
            
        cid = self.pools[runtime].popleft()
        try:
            c = self.docker.containers.get(cid)
            c.unpause()
            # ì‚¬ìš©í–ˆìœ¼ë‹ˆ ì±„ì›Œë†“ê¸° (Asyncë¡œ í•˜ë©´ ë” ì¢‹ìŒ)
            self._create_warm_container(runtime)
            return c
        except Exception:
            # ì‹¤íŒ¨í•˜ë©´ ì¬ì‹œë„
            return self._acquire_container(runtime)

    def _prepare_workspace(self, task: TaskMessage) -> Path:
        """S3 ë‹¤ìš´ë¡œë“œ ë° Zip Slip ë°©ì§€ ì••ì¶• í•´ì œ (ê¸°ì¡´ s3_service ë¡œì§)"""
        local_dir = Path(self.cfg["TASK_BASE_DIR"]) / task.request_id
        if local_dir.exists(): shutil.rmtree(local_dir)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        zip_path = local_dir / "code.zip"
        self.s3.download_file(self.cfg["S3_CODE_BUCKET"], task.s3_key, str(zip_path))
        
        with zipfile.ZipFile(zip_path, "r") as zf:
            for member in zf.namelist():
                # Zip Slip ë³´ì•ˆ ê²€ì‚¬
                target_path = (local_dir / member).resolve()
                if not str(target_path).startswith(str(local_dir.resolve())):
                    continue
                zf.extract(member, local_dir)
        
        zip_path.unlink()
        return local_dir

    def _upload_outputs(self, request_id: str, host_dir: Path) -> List[str]:
        """ê²°ê³¼ íŒŒì¼ S3 ì—…ë¡œë“œ (ê¸°ì¡´ output_uploader ë¡œì§)"""
        uploaded = []
        output_dir = host_dir / "output"
        if not output_dir.exists(): return []
        
        bucket = self.cfg.get("S3_USER_DATA_BUCKET")
        if not bucket: return []

        for file in output_dir.rglob("*"):
            if file.is_file():
                key = f"outputs/{request_id}/{file.name}"
                self.s3.upload_file(str(file), bucket, key)
                uploaded.append(f"s3://{bucket}/{key}")
        return uploaded

    def run(self, task: TaskMessage) -> ExecutionResult:
        """ì‘ì—… ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
        container = None
        host_work_dir = None
        start_time = time.time()
        
        try:
            # 1. ì‘ì—… ê³µê°„ ì¤€ë¹„
            host_work_dir = self._prepare_workspace(task)
            container_work_dir = f"{self.cfg['DOCKER_WORK_DIR_ROOT']}/{task.request_id}"

            # 2. ì»¨í…Œì´ë„ˆ íšë“
            container = self._acquire_container(task.runtime)
            
            # 3. ì‹¤í–‰ ì»¤ë§¨ë“œ êµ¬ì„±
            cmd = []
            if task.runtime == "python": cmd = ["python", "main.py"]
            elif task.runtime == "cpp": cmd = ["sh", "-c", "g++ main.cpp -o main && ./main"]
            elif task.runtime == "nodejs": cmd = ["node", "index.js"]

            # 4. ì‹¤í–‰
            exit_code, (stdout, stderr) = container.exec_run(
                cmd, workdir=container_work_dir, demux=True
            )
            
            stdout = stdout.decode('utf-8') if stdout else ""
            stderr = stderr.decode('utf-8') if stderr else ""
            
            # 5. ë©”íŠ¸ë¦­ ì¸¡ì •
            stats = container.stats(stream=False)
            usage = stats.get("memory_stats", {}).get("usage", 0)
            
            # 6. Auto-Tuning & CloudWatch
            tip = AutoTuner.analyze(usage, task.memory_mb)
            self.cw.publish_peak_memory(task.function_id, task.runtime, usage)
            
            # 7. íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = self._upload_outputs(task.request_id, host_work_dir)

            return ExecutionResult(
                request_id=task.request_id,
                success=(exit_code == 0),
                exit_code=exit_code,
                stdout=stdout,
                stderr=stderr,
                duration_ms=int((time.time() - start_time) * 1000),
                peak_memory_bytes=usage,
                optimization_tip=tip,
                output_files=uploaded_files
            )

        except Exception as e:
            logger.error("Execution failed", error=str(e))
            return ExecutionResult(
                request_id=task.request_id, success=False, exit_code=-1,
                stdout="", stderr=str(e), duration_ms=int((time.time() - start_time) * 1000)
            )
            
        finally:
            # ì •ë¦¬: ì»¨í…Œì´ë„ˆ íê¸° (ìƒíƒœ ì˜¤ì—¼ ë°©ì§€), í˜¸ìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ
            if container: 
                try: container.remove(force=True)
                except: pass
            if host_work_dir and host_work_dir.exists():
                shutil.rmtree(host_work_dir)